{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy heeft cats n dogs opnieuw gedaan met bijna alleen maar technieken die we tot nu toe hebben geleerd.\n",
    "* Cats n dogs\n",
    "    * Basics:\n",
    "        * dogscats-ensemble.ipynb\n",
    "        * Seperate vgg at boundary conv layers - dense layers, and before last layer.\n",
    "        * calculate outputs for training set at both places as precalc_mid en precalc_ll\n",
    "        * train last layer on precalc_ll\n",
    "        * replace trained last layer on vgg dense part\n",
    "        * Train dense part with with precalc_mid\n",
    "        * replace dense part on entire vgg\n",
    "        * Train dense part on entire set with data augmentation\n",
    "    * Extras, add batchnorm to vgg dense part:\n",
    "        * imagenet_batchnorm.ipynb\n",
    "        * add because you can use less dropout\n",
    "        * You CAN'T just stick batchnorm in pre trained network because it will change the weights for the next layer. What a batchnorm layer does is subtract mean and devide by standard deviation. What you CAN do is add a batchnorm layer that has mean and sd of the entire set it was originally trained on for that particular layer in the net. For the dense part of vgg that goes as follows:\n",
    "        * Calculate var and mu at the locations of the places where you will insert the two batchnorm layers (after the first two Dense layers)\n",
    "        * Create two batchnorm layers and insert them after the Dense layers\n",
    "        * Set the weights of the bn layers to the vars and mus\n",
    "        * Check if the predictions are still the same\n",
    "        * Extra: because the original net wasn't trained with batchnorm, it didn't take advantage of the fact it had it. You can retrain the new batchnormed dense part on the entire imagenet training set to improve the features. Done with pre computed conv layer it took less than an hour.\n",
    "        * You can now increase your learning rates because bn normalizes the activations, they won't escalate.\n",
    "* Collaborative filtering (INSANE)\n",
    "    * Check what the embeddings say about the movies. Embeddings are factors that calculate the rating for the movie, similar to weights in a nn. Embeddings are made up of bias terms and latent factors (OR embeddings and latent factors are the same and biast terms just add up OR multiply the entire latent factors of an input). Check lesson4.ipynb for how to implement the recommendation set data for a neural network\n",
    "        * If you just check the movies with the highest bias terms it gives  you some form of a normalized general rating, adjusted for the kinds of users there are.\n",
    "        * If you look at the latent factors, it is not necessarily clear what it is. You can apply PCA, to get three columns which and sort the best and worst of each PCA component.\n",
    "    * Keras functional API (versus sequential)\n",
    "        * With the functional model you can re-create the same thing a Sequential model does but you can also create different streams of data within the same model such as movies and users.\n",
    "        * A functional model starts with an input layer\n",
    "        * The first layer then is made and immediately after that, the inputs from the previous layer is called.\n",
    "        * The same goes for all following layers but now you can have for instance that data from an old layer moves into a layer a couple of steps further or new data is added halfway te network\n",
    "        * In the example of movie ratings:\n",
    "            * create a embedding/latentfactor model that makes n_factors of extra weights for each of the number of movies/users. Possibly with regularization. It also creates an input layer.\n",
    "            * create a bias model that takes input of the previous layer and create 1 factor/extra weight. Add flatten want handschoen.\n",
    "            * merge the user * movie factors with a dot product to get a rating for each movie per user. Flatten.\n",
    "            * Add bias for users\n",
    "            * Add bias for movies\n",
    "            * Set input, compile and fit them. This model thus has two inputs instead of one like we are used to.\n",
    "            * All *embedding* does is equivalent to one-hot encode but then just select relevant part of the matrix. \n",
    "        * With a functional model you could add meta data to a model. For instance after the convolutional layers so that it can be used by the dense part of the model.\n",
    "* NLP, natural language processing\n",
    "    * Implementation of the imdb review sentiment dataset\n",
    "        * Make it simpler; reduce the vocabulary. The word list is ordered by frequency. All words with a higher number than ie 5000 gets 5000.\n",
    "        * Average review length is 237 so lets make all reviews 500. You padd the reviews that are shorter with 0's from the front with the function .pad_sequences.\n",
    "        * Always create the simplest possible model; linear model of single hidden layer\n",
    "            * embedding(ie 32 embeddings for a 5000 word library); flatten; dense; dropout; dense sigmoid\n",
    "        * Single conv layer with max pooling because a sentence has order\n",
    "            * Embedding; dropout; conv1d (len is 5); dropout; maxpooling; flatten; dense; dropout; dense\n",
    "            * dropout in Embedding() means that it cancels some of the embedding characteristics. Dropout after embedding cancels entire words\n",
    "    * Interesting difference between image recognition and NLP is that in imagas, dogs look very similar from different viewpoints etc and words are just words, you cant look at them from a different angel or something. This means that you don't have to save and share entire networks, pre traind embeddings/wordvectors are enough\n",
    "        * You can download pre-trained word vectors.\n",
    "        * Depending on what the wordvector is trained on influences the wordvector obviously; skull is different in shakespeare than in some archeology text.\n",
    "        * GloVe, word2vec (from TF docs on word2vec is good)\n",
    "        * GloVe is pretrained on several databases, wikipedia, Twitter, Internet, etc. and several amounts of words in the vocabulary. You can often choose the size of the word vector.\n",
    "        * GloVe choose words smart to train; making ., etc words\n",
    "        * GloVe used Unsupervised learning to make sense of all the data. They took a sentence of n words, and replaced the middle word with something random. The original sentence is 'good', the new one 'bad'.\n",
    "    * Implementation; for NLP; ALWAYS use embeddings that are learned for you.\n",
    "        * TMSE is a dimentional reduction algorithm\n",
    "        * You need to remap wordidxs to GloVe\n",
    "        * Don't train embeddings, do train the rest of the single conv layer network.\n",
    "        * Train also the embeddings.\n",
    "* Multi-size CNN\n",
    "    * Use various filter sizes and concatenate the models\n",
    "* LSTM/RNN\n",
    "    * Attentional model; RNN that can decide where to look next in a picutre. Works well for ie large images for which you don't have the capacity to convolve over the entire image.\n",
    "    * Swiftkey; RNN.\n",
    "    * Generate random math proof in LaTeX wit RNN.\n",
    "    * Intuition RNN's\n",
    "        * If you want to analyse a sentence, you would want to understand something about order and memory that convolution is not good at understanding. In a Sequential model you would add a word, compute, add the next word, compute, etc. But you could also add a string of words and look at itself a couple of times before you say something about sentiment. I don't really get this yet.\n",
    "\n",
    "You only need to recompile a model if you changed the structure. No harm in re-compiling but mostly not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

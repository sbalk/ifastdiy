{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgg16stijn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vgg16stijn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vgg16stijn.py\n",
    "import json\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Lambda\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "# from keras.callbacks import CSVLogger\n",
    "\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n",
    "def RGB_to_BGR(x):\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr\n",
    "        \n",
    "class Vgg16Stijn():\n",
    "    def __init__(self): # sets up neural network\n",
    "        self.FILE_PATH = 'http://files.fast.ai/models/'\n",
    "        self.create()\n",
    "        self.get_classes()\n",
    "    \n",
    "    def create(self): # build actual network\n",
    "        \"\"\"\n",
    "            Builds neural network and loads pretrained vgg16 model weights\n",
    "        \"\"\"\n",
    "        model = self.model = Sequential()\n",
    "        model.add(Lambda(RGB_to_BGR, input_shape=(3,224,224), output_shape=(3,224,224))) #change rgb to bgr\n",
    "        \n",
    "        self.add_convlayers(2, 64)\n",
    "        self.add_convlayers(2, 128)\n",
    "        self.add_convlayers(3, 256)\n",
    "        self.add_convlayers(3, 512)\n",
    "        self.add_convlayers(3, 512)\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        self.add_fullconnect()\n",
    "        self.add_fullconnect()\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "        fname = 'vgg16.h5'\n",
    "        model.load_weights(get_file(fname, self.FILE_PATH+fname, cache_subdir='models'))\n",
    "    \n",
    "    def add_convlayers(self, layers, filters):\n",
    "        \"\"\"\n",
    "            Adds a specified number of ZeroPadding (line of zeros around image) and Covolution layers\n",
    "            to the model, and a MaxPooling (outputs max value of group) layer at the very end.\n",
    "\n",
    "            Args:\n",
    "                layers (int):   The number of zero padded convolution layers\n",
    "                                to be added to the model.\n",
    "                filters (int):  The number of convolution filters to be \n",
    "                                created for each layer. I think these are the \n",
    "                                number of learnable features per layer\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        for i in range(layers):\n",
    "            model.add(ZeroPadding2D((1, 1)))\n",
    "            model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    def add_fullconnect(self):\n",
    "        \"\"\"\n",
    "            Adds a fully connected layer of 4096 neurons (with 64x64 dimension) to the model with a\n",
    "            Dropout of 0.5. Dropout is random 0.5 of the input nodes are set to 0 to prevent overfitting.\n",
    "\n",
    "            Args:   None\n",
    "            Returns:   None\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.add(Dense(4096, activation='relu')) # fully connected layer with 64x64 dimension\n",
    "        model.add(Dropout(0.5)) # random half of the input nodes are set to 0 to prevent overfitting\n",
    "    \n",
    "    def get_classes(self):\n",
    "        fname = 'imagenet_class_index.json'\n",
    "        fpath = get_file(fname, self.FILE_PATH+fname, cache_subdir='models')\n",
    "        with open(fpath) as f:\n",
    "            class_dict = json.load(f)\n",
    "        self.classes = [class_dict[str(i)][1] for i in range(len(class_dict))]\n",
    "        \n",
    "    def batch_iterator(self, path, gen=image.ImageDataGenerator(), shuffle=True, batch_size=8, class_mode='categorical'):\n",
    "        return gen.flow_from_directory(directory=path, target_size=(224,224),\n",
    "                    class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "    def change_number_outputnodes(self, num):\n",
    "        model = self.model\n",
    "        model.pop()\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        model.add(Dense(num,activation='softmax'))\n",
    "        self.compile()\n",
    "        \n",
    "    def compile(self, lr=0.001):\n",
    "        \"\"\"\n",
    "            Configures the model for training.\n",
    "            See Keras documentation: https://keras.io/models/model/\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=Adam(lr=lr),\n",
    "                loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    def correct_class_id(self, batches):\n",
    "        \"\"\"\n",
    "            Adjusts final layer to correct number of output nodes and updates class labels.\n",
    "            \n",
    "            Args:\n",
    "                batches: keras.preprocessing.image.ImageDataGenerator opbject with flow_from_directory()\n",
    "        \"\"\"\n",
    "        self.change_number_outputnodes(batches.nb_class)\n",
    "        classes = list(iter(batches.class_indices)) \n",
    "        # creates list of classes from dict item 'batches.class_indices'\n",
    "        # batches.class_indices i.e. {'invasive': 1, 'non_invasive': 0}\n",
    "        for key in batches.class_indices:\n",
    "            classes[batches.class_indices[key]] = key\n",
    "        # orders class keys by value in dict like ['non_invasive', 'invasive'] because key 'non_invasive' has value 0\n",
    "        self.classes = classes\n",
    "        \n",
    "    def fit(self, batches, val_batches, nb_epoch=1, extra_info=''):\n",
    "        \"\"\"\n",
    "            Fits the model on data yielded batch-by-batch by a Python generator. \n",
    "            Saves metrics to extra_info+'training.log'.\n",
    "            See Keras documentation: https://keras.io/models/model/\n",
    "        \"\"\"\n",
    "#         csv_logger = CSVLogger(extra_info+'training.log', append=True)\n",
    "#         weight_save_callback = ModelCheckpoint('/path/to/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0,save_best_only=False, mode='auto')\n",
    "#         model.fit(X_train,y_train,batch_size=batch_size,nb_epoch=nb_epoch,callbacks=[weight_save_callback])\n",
    "        self.model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch,\n",
    "                validation_data=val_batches, nb_val_samples=val_batches.nb_sample) #, callbacks=[csv_logger])\n",
    "    \n",
    "    def fit_n_save_all(self, batches, val_batches, nb_epoch, results_path, extra_info=''):\n",
    "        \"\"\"\n",
    "            Uses fit and saves weights every epoch\n",
    "            \n",
    "            Args:\n",
    "                batches, val_batches, nb_epoch\n",
    "                extra_info: string attached to filename\n",
    "        \"\"\"\n",
    "        latest_weights_filename = ''\n",
    "        for epoch in range(nb_epoch):\n",
    "            print('Epoch %d' %epoch)\n",
    "            self.fit(batches, val_batches, nb_epoch=1)\n",
    "            latest_weights_filename = 'ft%d%s.h5' %(epoch+1, extra_info)\n",
    "            self.model.save_weights(results_path+latest_weights_filename)\n",
    "        print(\"Completed fit operations and saved in \" + str(latest_weights_filename))\n",
    "    \n",
    "    def test(self, path, batch_size=8):\n",
    "        \"\"\"\n",
    "            Predicts the classes using the trained model on data yielded batch-by-batch.\n",
    "\n",
    "            Args:\n",
    "                path (string):  Path to the target directory. It should contain one subdirectory \n",
    "                                per class.\n",
    "                batch_size (int): The number of images to be considered in each batch.\n",
    "            \n",
    "            Returns:\n",
    "                test_batches, numpy array(s) of predictions for the test_batches.\n",
    "    \n",
    "        \"\"\"\n",
    "        test_batches = self.batch_iterator(path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "        return test_batches, self.model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# %load vgg16.py\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# In case we are going to use the TensorFlow backend we need to explicitly set the Theano image ordering\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n",
    "def vgg_preprocess(x):\n",
    "    \"\"\"\n",
    "        Subtracts the mean RGB value, and transposes RGB to BGR.\n",
    "        The mean RGB was computed on the image set used to train the VGG model.\n",
    "\n",
    "        Args: \n",
    "            x: Image array (height x width x channels)\n",
    "        Returns:\n",
    "            Image array (height x width x transposed_channels)\n",
    "    \"\"\"\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr\n",
    "\n",
    "\n",
    "class Vgg16():\n",
    "    \"\"\"\n",
    "        The VGG 16 Imagenet model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.FILE_PATH = 'http://files.fast.ai/models/'\n",
    "        self.create()\n",
    "        self.get_classes()\n",
    "\n",
    "\n",
    "    def get_classes(self):\n",
    "        \"\"\"\n",
    "            Downloads the Imagenet classes index file and loads it to self.classes.\n",
    "            The file is downloaded only if it not already in the cache.\n",
    "        \"\"\"\n",
    "        fname = 'imagenet_class_index.json'\n",
    "        fpath = get_file(fname, self.FILE_PATH+fname, cache_subdir='models')\n",
    "        with open(fpath) as f:\n",
    "            class_dict = json.load(f)\n",
    "        self.classes = [class_dict[str(i)][1] for i in range(len(class_dict))]\n",
    "\n",
    "    def predict(self, imgs, details=False):\n",
    "        \"\"\"\n",
    "            Predict the labels of a set of images using the VGG16 model.\n",
    "\n",
    "            Args:\n",
    "                imgs (ndarray)    : An array of N images (size: N x width x height x channels).\n",
    "                details : ??\n",
    "            \n",
    "            Returns:\n",
    "                preds (np.array) : Highest confidence value of the predictions for each image.\n",
    "                idxs (np.ndarray): Class index of the predictions with the max confidence.\n",
    "                classes (list)   : Class labels of the predictions with the max confidence.\n",
    "        \"\"\"\n",
    "        # predict probability of each class for each image\n",
    "        all_preds = self.model.predict(imgs)\n",
    "        # for each image get the index of the class with max probability\n",
    "        idxs = np.argmax(all_preds, axis=1)\n",
    "        # get the values of the highest probability for each image\n",
    "        preds = [all_preds[i, idxs[i]] for i in range(len(idxs))]\n",
    "        # get the label of the class with the highest probability for each image\n",
    "        classes = [self.classes[idx] for idx in idxs]\n",
    "        return np.array(preds), idxs, classes\n",
    "\n",
    "\n",
    "    def ConvBlock(self, layers, filters):\n",
    "        \"\"\"\n",
    "            Adds a specified number of ZeroPadding and Covolution layers\n",
    "            to the model, and a MaxPooling layer at the very end.\n",
    "\n",
    "            Args:\n",
    "                layers (int):   The number of zero padded convolution layers\n",
    "                                to be added to the model.\n",
    "                filters (int):  The number of convolution filters to be \n",
    "                                created for each layer.\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        for i in range(layers):\n",
    "            model.add(ZeroPadding2D((1, 1)))\n",
    "            model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "    def FCBlock(self):\n",
    "        \"\"\"\n",
    "            Adds a fully connected layer of 4096 neurons to the model with a\n",
    "            Dropout of 0.5\n",
    "\n",
    "            Args:   None\n",
    "            Returns:   None\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"\n",
    "            Creates the VGG16 network achitecture and loads the pretrained weights.\n",
    "\n",
    "            Args:   None\n",
    "            Returns:   None\n",
    "        \"\"\"\n",
    "        model = self.model = Sequential()\n",
    "        model.add(Lambda(vgg_preprocess, input_shape=(3,224,224), output_shape=(3,224,224)))\n",
    "\n",
    "        self.ConvBlock(2, 64)\n",
    "        self.ConvBlock(2, 128)\n",
    "        self.ConvBlock(3, 256)\n",
    "        self.ConvBlock(3, 512)\n",
    "        self.ConvBlock(3, 512)\n",
    "\n",
    "        model.add(Flatten())\n",
    "        self.FCBlock()\n",
    "        self.FCBlock()\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "        fname = 'vgg16.h5'\n",
    "        model.load_weights(get_file(fname, self.FILE_PATH+fname, cache_subdir='models'))\n",
    "\n",
    "\n",
    "    def get_batches(self, path, gen=image.ImageDataGenerator(), shuffle=True, batch_size=8, class_mode='categorical'):\n",
    "        \"\"\"\n",
    "            Takes the path to a directory, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.\n",
    "\n",
    "            See Keras documentation: https://keras.io/preprocessing/image/\n",
    "        \"\"\"\n",
    "        return gen.flow_from_directory(path, target_size=(224,224),\n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    def ft(self, num):\n",
    "        \"\"\"\n",
    "            Replace the last layer of the model with a Dense (fully connected) layer of num neurons.\n",
    "            Will also lock the weights of all layers except the new layer so that we only learn\n",
    "            weights for the last layer in subsequent training.\n",
    "\n",
    "            Args:\n",
    "                num (int) : Number of neurons in the Dense layer\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.pop()\n",
    "        for layer in model.layers: layer.trainable=False\n",
    "        model.add(Dense(num, activation='softmax'))\n",
    "        self.compile()\n",
    "\n",
    "    def finetune(self, batches):\n",
    "        \"\"\"\n",
    "            Modifies the original VGG16 network architecture and updates self.classes for new training data.\n",
    "            \n",
    "            Args:\n",
    "                batches : A keras.preprocessing.image.ImageDataGenerator object.\n",
    "                          See definition for get_batches().\n",
    "        \"\"\"\n",
    "        self.ft(batches.nb_class)\n",
    "        classes = list(iter(batches.class_indices)) # get a list of all the class labels\n",
    "        # batches.class_indices is a dict with the class name as key and an index as value\n",
    "        # eg. {'cats': 0, 'dogs': 1}\n",
    "\n",
    "        # sort the class labels by index according to batches.class_indices and update model.classes\n",
    "        for c in batches.class_indices:\n",
    "            classes[batches.class_indices[c]] = c\n",
    "        self.classes = classes\n",
    "\n",
    "\n",
    "    def compile(self, lr=0.001):\n",
    "        \"\"\"\n",
    "            Configures the model for training.\n",
    "            See Keras documentation: https://keras.io/models/model/\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=Adam(lr=lr),\n",
    "                loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def fit_data(self, trn, labels,  val, val_labels,  nb_epoch=1, batch_size=64):\n",
    "        \"\"\"\n",
    "            Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "            See Keras documentation: https://keras.io/models/model/\n",
    "        \"\"\"\n",
    "        self.model.fit(trn, labels, nb_epoch=nb_epoch,\n",
    "                validation_data=(val, val_labels), batch_size=batch_size)\n",
    "\n",
    "\n",
    "    def fit(self, batches, val_batches, nb_epoch=1):\n",
    "        \"\"\"\n",
    "            Fits the model on data yielded batch-by-batch by a Python generator.\n",
    "            See Keras documentation: https://keras.io/models/model/\n",
    "        \"\"\"\n",
    "        self.model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch,\n",
    "                validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "\n",
    "    def test(self, path, batch_size=8):\n",
    "        \"\"\"\n",
    "            Predicts the classes using the trained model on data yielded batch-by-batch.\n",
    "\n",
    "            Args:\n",
    "                path (string):  Path to the target directory. It should contain one subdirectory \n",
    "                                per class.\n",
    "                batch_size (int): The number of images to be considered in each batch.\n",
    "            \n",
    "            Returns:\n",
    "                test_batches, numpy array(s) of predictions for the test_batches.\n",
    "    \n",
    "        \"\"\"\n",
    "        test_batches = self.get_batches(path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "        return test_batches, self.model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%writefile utils_anders.py\n",
    "# %load utils.py\n",
    "\n",
    "from __future__ import division,print_function\n",
    "import math, os, json, sys, re\n",
    "import cPickle as pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from numpy.random import random, permutation, randn, normal, uniform, choice\n",
    "from numpy import newaxis\n",
    "import scipy\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy.ndimage import imread\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import bcolz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.lib.display import FileLink\n",
    "\n",
    "import theano\n",
    "from theano import shared, tensor as T\n",
    "from theano.tensor.nnet import conv2d, nnet\n",
    "from theano.tensor.signal import pool\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2, activity_l2, l1, activity_l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from vgg16 import *\n",
    "from vgg16bn import *\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "\n",
    "\n",
    "to_bw = np.array([0.299, 0.587, 0.114])\n",
    "\n",
    "def gray(img):\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        return np.rollaxis(img, 0, 1).dot(to_bw)\n",
    "    else:\n",
    "        return np.rollaxis(img, 0, 3).dot(to_bw)\n",
    "\n",
    "def to_plot(img):\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        return np.rollaxis(img, 0, 1).astype(np.uint8)\n",
    "    else:\n",
    "        return np.rollaxis(img, 0, 3).astype(np.uint8)\n",
    "\n",
    "def plot(img):\n",
    "    plt.imshow(to_plot(img))\n",
    "\n",
    "\n",
    "def floor(x):\n",
    "    return int(math.floor(x))\n",
    "def ceil(x):\n",
    "    return int(math.ceil(x))\n",
    "\n",
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "\n",
    "def plots2(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    print('1', 'type(ims) = ',type(ims), ims.shape)\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        print('2')\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    print('3')\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "        print('4',i)\n",
    "    print('5')\n",
    "\n",
    "def do_clip(arr, mx):\n",
    "    clipped = np.clip(arr, (1-mx)/1, mx)\n",
    "    return clipped/clipped.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def onehot(x):\n",
    "    return to_categorical(x)\n",
    "\n",
    "\n",
    "def wrap_config(layer):\n",
    "    return {'class_name': layer.__class__.__name__, 'config': layer.get_config()}\n",
    "\n",
    "\n",
    "def copy_layer(layer): return layer_from_config(wrap_config(layer))\n",
    "\n",
    "\n",
    "def copy_layers(layers): return [copy_layer(layer) for layer in layers]\n",
    "\n",
    "\n",
    "def copy_weights(from_layers, to_layers):\n",
    "    for from_layer,to_layer in zip(from_layers, to_layers):\n",
    "        to_layer.set_weights(from_layer.get_weights())\n",
    "\n",
    "\n",
    "def copy_model(m):\n",
    "    res = Sequential(copy_layers(m.layers))\n",
    "    copy_weights(m.layers, res.layers)\n",
    "    return res\n",
    "\n",
    "\n",
    "def insert_layer(model, new_layer, index):\n",
    "    res = Sequential()\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if i==index: res.add(new_layer)\n",
    "        copied = layer_from_config(wrap_config(layer))\n",
    "        res.add(copied)\n",
    "        copied.set_weights(layer.get_weights())\n",
    "    return res\n",
    "\n",
    "\n",
    "def adjust_dropout(weights, prev_p, new_p):\n",
    "    scal = (1-prev_p)/(1-new_p)\n",
    "    return [o*scal for o in weights]\n",
    "\n",
    "\n",
    "def get_data(path, target_size=(224,224)):\n",
    "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None, target_size=target_size)\n",
    "    return np.concatenate([batches.next() for i in range(batches.nb_sample)])\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    (This function is copied from the scikit docs.)\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "\n",
    "def mk_size(img, r2c):\n",
    "    r,c,_ = img.shape\n",
    "    curr_r2c = r/c\n",
    "    new_r, new_c = r,c\n",
    "    if r2c>curr_r2c:\n",
    "        new_r = floor(c*r2c)\n",
    "    else:\n",
    "        new_c = floor(r/r2c)\n",
    "    arr = np.zeros((new_r, new_c, 3), dtype=np.float32)\n",
    "    r2=(new_r-r)//2\n",
    "    c2=(new_c-c)//2\n",
    "    arr[floor(r2):floor(r2)+r,floor(c2):floor(c2)+c] = img\n",
    "    return arr\n",
    "\n",
    "\n",
    "def mk_square(img):\n",
    "    x,y,_ = img.shape\n",
    "    maxs = max(img.shape[:2])\n",
    "    y2=(maxs-y)//2\n",
    "    x2=(maxs-x)//2\n",
    "    arr = np.zeros((maxs,maxs,3), dtype=np.float32)\n",
    "    arr[floor(x2):floor(x2)+x,floor(y2):floor(y2)+y] = img\n",
    "    return arr\n",
    "\n",
    "\n",
    "def vgg_ft(out_dim):\n",
    "    vgg = Vgg16()\n",
    "    vgg.ft(out_dim)\n",
    "    model = vgg.model\n",
    "    return model\n",
    "\n",
    "def vgg_ft_bn(out_dim):\n",
    "    vgg = Vgg16BN()\n",
    "    vgg.ft(out_dim)\n",
    "    model = vgg.model\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_classes(path):\n",
    "    batches = get_batches(path+'train', shuffle=False, batch_size=1)\n",
    "    val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "    test_batches = get_batches(path+'test', shuffle=False, batch_size=1)\n",
    "    return (val_batches.classes, batches.classes, onehot(val_batches.classes), onehot(batches.classes),\n",
    "        val_batches.filenames, batches.filenames, test_batches.filenames)\n",
    "\n",
    "\n",
    "def split_at(model, layer_type):\n",
    "    layers = model.layers\n",
    "    layer_idx = [index for index,layer in enumerate(layers)\n",
    "                 if type(layer) is layer_type][-1]\n",
    "    return layers[:layer_idx+1], layers[layer_idx+1:]\n",
    "\n",
    "\n",
    "class MixIterator(object):\n",
    "    def __init__(self, iters):\n",
    "        self.iters = iters\n",
    "        self.multi = type(iters) is list\n",
    "        if self.multi:\n",
    "            self.N = sum([it[0].N for it in self.iters])\n",
    "        else:\n",
    "            self.N = sum([it.N for it in self.iters])\n",
    "\n",
    "    def reset(self):\n",
    "        for it in self.iters: it.reset()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self, *args, **kwargs):\n",
    "        if self.multi:\n",
    "            nexts = [[next(it) for it in o] for o in self.iters]\n",
    "            n0 = np.concatenate([n[0] for n in nexts])\n",
    "            n1 = np.concatenate([n[1] for n in nexts])\n",
    "            return (n0, n1)\n",
    "        else:\n",
    "            nexts = [next(it) for it in self.iters]\n",
    "            n0 = np.concatenate([n[0] for n in nexts])\n",
    "            n1 = np.concatenate([n[1] for n in nexts])\n",
    "            return (n0, n1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

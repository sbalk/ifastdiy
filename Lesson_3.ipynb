{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write up from youtube lesson 3\n",
    "PART 1\n",
    "* Try interactive for sliders in jupyter 17:00\n",
    "* Try deep visualisation toolbox by yosinki.com/deepvis\n",
    "* matt zeiler visualisation paper lezen omdat small filters met more layers beter werkt\n",
    "* neuralnetworksanddeeplearning.com/chapter4\n",
    "* vgg.model.summary()\n",
    "* trainable layers, how do you choose which ones are trainable? \n",
    "* 1:00:00 training layers, how far do you go back? (1:05:00) Intuition OR Look at the visualisation (visualisation toolbox) of the layers and see which ones are defining relevant information OR experiment. Conv layers are for space relationships, spacely related to other stuff. If you're already looking for sub catergories, only do the last layer (cats and dogs), if more is on; train more layers.\n",
    "* 41:12 convolve and corrolate are similar/same\n",
    "* 57:30 eye does voveation, focus op bepaalde dingen, attentional models\n",
    "* 47:30 softmax gebruiken omdat het goed is in one-hot, if you have multiple occurrences, use something else\n",
    "PART 2\n",
    "* 1:10:00 512 filters in de laatste layer zijn niet 512 specifieke dingen maar 512 complex functions, die complexe dingen herkennen. Layer 6 van deepvis herkent gezichten dus moeilijk in te schatten wat hogere layers begrijpen.\n",
    "* Overfitting is recognized if training set has much higher acc than your validation/test set\n",
    "* Underfitting is recognized if training error is much lower than validation error\n",
    "* 1:15:00 dropout makes it possible to create giant networks without overfitting because it sets x (0<x<1) of the weights in the network to 0\n",
    "* In cats and dogs, underfitting occurs; this is because dropout is set to 0.5 which is too much for cats and dogs\n",
    "* Every time you write a function you use more than once; put it in utils or w-ever\n",
    "* 1:18:00 Removing dropout: J slaat het model op tm laatste conv layer en maakt een nieuw nn eraan vast\n",
    "* 1:22:31 ! If you remove all dropouts, you need to devide the weights by two to make it the same as your original model\n",
    "* 1:27:00 You want to dropout less in the earlier layers because you loose that information for later layers too. Dropout 0.1 then 0.2 then 0.3 etc.\n",
    "* 1:28:00 Reduce overfitting:\n",
    "    1. Add more data\n",
    "    2. Use data augmentation\n",
    "        * Rotate and shift, color(channel in Keras original data, don't do it on validation. THINK what is usefull for your test set\n",
    "        * add gen to get_batches\n",
    "        * how much augmentation? Use intuition and experiment\n",
    "        * if you start overfitting, you're using too much augmentation\n",
    "        * val acc moving around because the set is small\n",
    "        * batch normalisation, always do it, it means average around 0, increases speed, reduce overfitting\n",
    "    3. Use architectures that generalize well\n",
    "    4. Add regularization\n",
    "        * Dropout or L1, L2 kind of approach\n",
    "    5. Reduce architecture complexity.\n",
    "* 1:49:00 MNIST \n",
    "* 2:00:00 Make sure that you get the same acc on val as on test if you submit to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
